
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorial/task_eval.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorial_task_eval.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorial_task_eval.py:


.. _eval:

æ™ºèƒ½ä½“è¯„æµ‹
=========================

AgentScope æä¾›äº†ä¸€ä¸ªå†…ç½®çš„è¯„æµ‹æ¡†æ¶ï¼Œç”¨äºè¯„æµ‹æ™ºèƒ½ä½“åœ¨ä¸åŒä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š

- åŸºäº `Ray <https://github.com/ray-project/ray>`_ çš„å¹¶è¡Œå’Œåˆ†å¸ƒå¼è¯„ä¼°
- æ”¯æŒä¸­æ–­åç»§ç»­è¯„ä¼°
- [å¼€å‘ä¸­] è¯„ä¼°ç»“æœå¯è§†åŒ–

.. note:: æˆ‘ä»¬æ­£åœ¨æŒç»­é›†æˆæ–°çš„åŸºå‡†æµ‹è¯•åˆ° AgentScope ä¸­ï¼š

 - âœ… `ACEBench <https://github.com/ACEBench/ACEBench>`_
 - ğŸš§ `GAIA <https://huggingface.co/datasets/gaia-benchmark/GAIA/tree/main>`_ åŸºå‡†æµ‹è¯•


æ¦‚è¿°
---------------------------

AgentScope è¯„ä¼°æ¡†æ¶ç”±å‡ ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼š

- **åŸºå‡†æµ‹è¯• (Benchmark)**: ç”¨äºç³»ç»Ÿæ€§è¯„ä¼°çš„ä»»åŠ¡é›†åˆ
    - **ä»»åŠ¡ (Task)**: åŒ…å«è¾“å…¥ã€æ ‡å‡†ç­”æ¡ˆå’ŒæŒ‡æ ‡çš„ç‹¬ç«‹è¯„ä¼°å•å…ƒ
        - **æŒ‡æ ‡ (Metric)**: è¯„ä¼°è§£å†³æ–¹æ¡ˆè´¨é‡çš„æµ‹é‡å‡½æ•°
- **è¯„ä¼°å™¨ (Evaluator)**: è¿è¡Œè¯„ä¼°çš„å¼•æ“ï¼Œèšåˆç»“æœå¹¶åˆ†ææ€§èƒ½
    - **è¯„ä¼°å™¨å­˜å‚¨ (Evaluator Storage)**: ç”¨äºè®°å½•å’Œæ£€ç´¢è¯„ä¼°ç»“æœçš„æŒä¹…åŒ–å­˜å‚¨
- **è§£å†³æ–¹æ¡ˆ (Solution)**: ç”¨æˆ·å®šä¹‰çš„è§£å†³æ–¹æ¡ˆ

.. figure:: ../../_static/images/evaluation.png
    :width: 90%
    :alt: AgentScope è¯„ä¼°æ¡†æ¶

    *AgentScope è¯„ä¼°æ¡†æ¶*

AgentScope å½“å‰çš„å®ç°åŒ…æ‹¬ï¼š

- è¯„ä¼°å™¨ï¼š
    - ``RayEvaluator``: åŸºäº ray çš„è¯„ä¼°å™¨ï¼Œæ”¯æŒå¹¶è¡Œå’Œåˆ†å¸ƒå¼è¯„ä¼°ã€‚
    - ``GeneralEvaluator``: é€šç”¨è¯„ä¼°å™¨ï¼ŒæŒ‰é¡ºåºè¿è¡Œä»»åŠ¡ï¼Œä¾¿äºè°ƒè¯•ã€‚
- åŸºå‡†æµ‹è¯•ï¼š
    - ``ACEBench``: ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚

æˆ‘ä»¬åœ¨ `GitHub ä»“åº“ <https://github.com/agentscope-ai/agentscope/tree/main/examples/evaluation/ace_bench>`_ ä¸­æä¾›äº†ä¸€ä¸ªä½¿ç”¨ ``RayEvaluator`` å’Œ ACEBench ä¸­æ™ºèƒ½ä½“å¤šæ­¥éª¤ä»»åŠ¡çš„ç©å…·ç¤ºä¾‹ã€‚

æ ¸å¿ƒç»„ä»¶
---------------
æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç®€å•çš„ç©å­¦é—®é¢˜åŸºå‡†æµ‹è¯•æ¥æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ AgentScope è¯„ä¼°æ¨¡å—ã€‚

.. GENERATED FROM PYTHON SOURCE LINES 52-74

.. code-block:: Python


    TOY_BENCHMARK = [
        {
            "id": "math_problem_1",
            "question": "What is 2 + 2?",
            "ground_truth": 4.0,
            "tags": {
                "difficulty": "easy",
                "category": "math",
            },
        },
        {
            "id": "math_problem_2",
            "question": "What is 12345 + 54321 + 6789 + 9876?",
            "ground_truth": 83331,
            "tags": {
                "difficulty": "medium",
                "category": "math",
            },
        },
    ]








.. GENERATED FROM PYTHON SOURCE LINES 75-81

ä»ä»»åŠ¡ã€è§£å†³æ–¹æ¡ˆå’ŒæŒ‡æ ‡åˆ°åŸºå‡†æµ‹è¯•
~~~~~~~~~~~~~~~~~~~

- ä¸€ä¸ª ``SolutionOutput`` (Agentè§£å†³æ–¹æ¡ˆè¾“å‡º) åŒ…å«æ™ºèƒ½ä½“ç”Ÿæˆçš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬è½¨è¿¹å’Œæœ€ç»ˆè¾“å‡ºã€‚
- ä¸€ä¸ª ``Metric`` (è¯„æµ‹æŒ‡æ ‡) ä»£è¡¨ä¸€ä¸ªå•ä¸€çš„è¯„ä¼°å¯è°ƒç”¨å®ä¾‹ï¼Œå®ƒå°†ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆï¼ˆä¾‹å¦‚ï¼Œè½¨è¿¹æˆ–æœ€ç»ˆè¾“å‡ºï¼‰ä¸æ ‡å‡†ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒã€‚
åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæŒ‡æ ‡ï¼Œç®€å•åœ°æ£€æŸ¥è§£å†³æ–¹æ¡ˆä¸­çš„ ``output`` å­—æ®µæ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆåŒ¹é…ã€‚

.. GENERATED FROM PYTHON SOURCE LINES 81-121

.. code-block:: Python


    from agentscope.evaluate import (
        SolutionOutput,
        MetricBase,
        MetricResult,
        MetricType,
    )


    class CheckEqual(MetricBase):
        def __init__(
            self,
            ground_truth: float,
        ):
            super().__init__(
                name="math check number equal",
                metric_type=MetricType.NUMERICAL,
                description="Toy metric checking if two numbers are equal",
                categories=[],
            )
            self.ground_truth = ground_truth

        def __call__(
            self,
            solution: SolutionOutput,
        ) -> MetricResult:
            if solution.output == self.ground_truth:
                return MetricResult(
                    name=self.name,
                    result=1.0,
                    message="Correct",
                )
            else:
                return MetricResult(
                    name=self.name,
                    result=0.0,
                    message="Incorrect",
                )









.. GENERATED FROM PYTHON SOURCE LINES 122-124

- ä¸€ä¸ª ``Task`` (ä»»åŠ¡) æ˜¯åŸºå‡†æµ‹è¯•ä¸­çš„ä¸€ä¸ªå•å…ƒï¼ŒåŒ…å«æ™ºèƒ½ä½“æ‰§è¡Œå’Œè¯„ä¼°æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œè¾“å…¥/æŸ¥è¯¢åŠå…¶æ ‡å‡†ç­”æ¡ˆï¼‰ã€‚
- ä¸€ä¸ª ``Benchmark`` (åŸºå‡†æµ‹è¯•) ç»„ç»‡å¤šä¸ªä»»åŠ¡è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ã€‚

.. GENERATED FROM PYTHON SOURCE LINES 124-172

.. code-block:: Python


    from typing import Generator
    from agentscope.evaluate import (
        Task,
        BenchmarkBase,
    )


    class ToyBenchmark(BenchmarkBase):
        def __init__(self):
            super().__init__(
                name="Toy bench",
                description="A toy benchmark for demonstrating the evaluation module.",
            )
            self.dataset = self._load_data()

        @staticmethod
        def _load_data() -> list[Task]:
            dataset = []
            for item in TOY_BENCHMARK:
                dataset.append(
                    Task(
                        id=item["id"],
                        input=item["question"],
                        ground_truth=item["ground_truth"],
                        tags=item.get("tags", {}),
                        metrics=[
                            CheckEqual(item["ground_truth"]),
                        ],
                        metadata={},
                    ),
                )
            return dataset

        def __iter__(self) -> Generator[Task, None, None]:
            """éå†åŸºå‡†æµ‹è¯•ã€‚"""
            for task in self.dataset:
                yield task

        def __getitem__(self, index: int) -> Task:
            """æ ¹æ®ç´¢å¼•è·å–ä»»åŠ¡ã€‚"""
            return self.dataset[index]

        def __len__(self) -> int:
            """è·å–åŸºå‡†æµ‹è¯•çš„é•¿åº¦ã€‚"""
            return len(self.dataset)









.. GENERATED FROM PYTHON SOURCE LINES 173-183

è¯„ä¼°å™¨
~~~~~~~~~~

è¯„ä¼°å™¨ (Evaluators) ç®¡ç†è¯„ä¼°è¿‡ç¨‹ã€‚å®ƒä»¬å¯ä»¥è‡ªåŠ¨éå†
åŸºå‡†æµ‹è¯•ä¸­çš„ä»»åŠ¡ï¼Œå¹¶å°†æ¯ä¸ªä»»åŠ¡è¾“å…¥åˆ°è§£å†³æ–¹æ¡ˆç”Ÿæˆå‡½æ•°ä¸­ï¼Œ
å¼€å‘è€…éœ€è¦åœ¨å…¶ä¸­å®šä¹‰è¿è¡Œæ™ºèƒ½ä½“å’Œæ£€ç´¢
æ‰§è¡Œç»“æœå’Œè½¨è¿¹çš„é€»è¾‘ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ª
ä½¿ç”¨æˆ‘ä»¬çš„ç©å…·åŸºå‡†æµ‹è¯•è¿è¡Œ ``GeneralEvaluator`` (é€šç”¨è¯„ä¼°å™¨) çš„ç¤ºä¾‹ã€‚å¦‚æœæœ‰ä¸€ä¸ªå¤§å‹
åŸºå‡†æµ‹è¯•ï¼Œå¼€å‘è€…å¸Œæœ›é€šè¿‡å¹¶è¡ŒåŒ–æ›´é«˜æ•ˆåœ°è¿›è¡Œè¯„ä¼°ï¼Œ
``RayEvaluator`` (Rayè¯„ä¼°å™¨) ä¹Ÿå¯ä½œä¸ºå†…ç½®è§£å†³æ–¹æ¡ˆä½¿ç”¨ã€‚

.. GENERATED FROM PYTHON SOURCE LINES 183-256

.. code-block:: Python



    import os
    import asyncio
    from typing import Callable
    from pydantic import BaseModel

    from agentscope.message import Msg
    from agentscope.model import DashScopeChatModel
    from agentscope.formatter import DashScopeChatFormatter
    from agentscope.agent import ReActAgent

    from agentscope.evaluate import (
        GeneralEvaluator,
        FileEvaluatorStorage,
    )


    class ToyBenchAnswerFormat(BaseModel):
        answer_as_number: float


    async def toy_solution_generation(
        task: Task,
        pre_hook: Callable,
    ) -> SolutionOutput:
        agent = ReActAgent(
            name="Friday",
            sys_prompt="You are a helpful assistant named Friday. "
            "Your target is to solve the given task with your tools. "
            "Try to solve the task as best as you can.",
            model=DashScopeChatModel(
                api_key=os.environ.get("DASHSCOPE_API_KEY"),
                model_name="qwen-max",
                stream=False,
            ),
            formatter=DashScopeChatFormatter(),
        )
        agent.register_instance_hook(
            "pre_print",
            "save_logging",
            pre_hook,
        )
        msg_input = Msg("user", task.input, role="user")
        res = await agent(
            msg_input,
            structured_model=ToyBenchAnswerFormat,
        )
        return SolutionOutput(
            success=True,
            output=res.metadata.get("answer_as_number", None),
            trajectory=[],
        )


    async def main() -> None:
        evaluator = GeneralEvaluator(
            name="ACEbench evaluation",
            benchmark=ToyBenchmark(),
            # é‡å¤å¤šå°‘æ¬¡
            n_repeat=1,
            storage=FileEvaluatorStorage(
                save_dir="./results",
            ),
            # ä½¿ç”¨å¤šå°‘ä¸ªå·¥ä½œè¿›ç¨‹
            n_workers=1,
        )

        # è¿è¡Œè¯„ä¼°
        await evaluator.run(toy_solution_generation)


    asyncio.run(main())




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Friday: The sum of 2 + 2 is 4.
    Friday: The sum of 12345, 54321, 6789, and 9876 is 83331.
    Repeat ID: 0
            Metric: math check number equal
                    Type: MetricType.NUMERICAL
                    Involved tasks: 2
                    Completed tasks: 2
                    Incomplete tasks: 0
                    Aggregation: {
                        "mean": 1.0,
                        "max": 1.0,
                        "min": 1.0
                    }





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 8.921 seconds)


.. _sphx_glr_download_tutorial_task_eval.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: task_eval.ipynb <task_eval.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: task_eval.py <task_eval.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: task_eval.zip <task_eval.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
