
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorial/task_model.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorial_task_model.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorial_task_model.py:


.. _model:

Model
====================

In this tutorial, we introduce the model APIs integrated in AgentScope, how to use them and how to integrate new model APIs.
The supported model APIs and providers include:

.. list-table::
    :header-rows: 1

    * - API
      - Class
      - Compatible
      - Streaming
      - Tools
      - Vision
      - Reasoning
    * - OpenAI
      - ``OpenAIChatModel``
      - vLLM, DeepSeek
      - âœ…
      - âœ…
      - âœ…
      - âœ…
    * - DashScope
      - ``DashScopeChatModel``
      -
      - âœ…
      - âœ…
      - âœ…
      - âœ…
    * - Anthropic
      - ``AnthropicChatModel``
      -
      - âœ…
      - âœ…
      - âœ…
      - âœ…
    * - Gemini
      - ``GeminiChatModel``
      -
      - âœ…
      - âœ…
      - âœ…
      - âœ…
    * - Ollama
      - ``OllamaChatModel``
      -
      - âœ…
      - âœ…
      - âœ…
      - âœ…

To provide unified model interfaces, the above model classes has the following common methods:

- The first three arguments of the ``__call__`` method are ``messages`` , ``tools`` and ``tool_choice``, representing the input messages, JSON schema of tool functions, and tool selection mode, respectively.
- The return type are either a ``ChatResponse`` instance or an async generator of ``ChatResponse`` in streaming mode.

.. note:: Different model APIs differ in the input message format, refer to :ref:`prompt` for more details.

The ``ChatResponse`` instance contains the generated thinking/text/tool use content, identity, created time and usage information.

.. GENERATED FROM PYTHON SOURCE LINES 66-91

.. code-block:: Python

    import asyncio
    import json
    import os

    from agentscope.message import TextBlock, ToolUseBlock, ThinkingBlock, Msg
    from agentscope.model import ChatResponse, DashScopeChatModel

    response = ChatResponse(
        content=[
            ThinkingBlock(
                type="thinking",
                thinking="I should search for AgentScope on Google.",
            ),
            TextBlock(type="text", text="I'll search for AgentScope on Google."),
            ToolUseBlock(
                type="tool_use",
                id="642n298gjna",
                name="google_search",
                input={"query": "AgentScope?"},
            ),
        ],
    )

    print(response)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ChatResponse(content=[{'type': 'thinking', 'thinking': 'I should search for AgentScope on Google.'}, {'type': 'text', 'text': "I'll search for AgentScope on Google."}, {'type': 'tool_use', 'id': '642n298gjna', 'name': 'google_search', 'input': {'query': 'AgentScope?'}}], id='2025-08-18 07:12:48.369_863e3c', created_at='2025-08-18 07:12:48.369', type='chat', usage=None)




.. GENERATED FROM PYTHON SOURCE LINES 92-93

Taking ``DashScopeChatModel`` as an example, we can use it to create a chat model instance and call it with messages and tools:

.. GENERATED FROM PYTHON SOURCE LINES 93-118

.. code-block:: Python



    async def example_model_call() -> None:
        """An example of using the DashScopeChatModel."""
        model = DashScopeChatModel(
            model_name="qwen-max",
            api_key=os.environ["DASHSCOPE_API_KEY"],
            stream=False,
        )

        res = await model(
            messages=[
                {"role": "user", "content": "Hi!"},
            ],
        )

        # You can directly create a ``Msg`` object with the response content
        msg_res = Msg("Friday", res.content, "assistant")

        print("The response:", res)
        print("The response as Msg:", msg_res)


    asyncio.run(example_model_call())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The response: ChatResponse(content=[{'type': 'text', 'text': 'Hello! How can I assist you today?'}], id='2025-08-18 07:12:49.552_f60ae4', created_at='2025-08-18 07:12:49.552', type='chat', usage=ChatUsage(input_tokens=10, output_tokens=9, time=1.182441, type='chat'))
    The response as Msg: Msg(id='FgbZKmmABeMGDxGpiNnQfx', name='Friday', content=[{'type': 'text', 'text': 'Hello! How can I assist you today?'}], role='assistant', metadata=None, timestamp='2025-08-18 07:12:49.552', invocation_id='None')




.. GENERATED FROM PYTHON SOURCE LINES 119-126

Streaming
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
To enable streaming model, set the ``stream`` parameter in the model constructor to ``True``.
When streaming is enabled, the ``__call__`` method will return an **async generator** that yields ``ChatResponse`` instances as they are generated by the model.

.. note:: The streaming mode in AgentScope is designed to be **cumulative**, meaning the content in each chunk contains all the previous content plus the newly generated content.


.. GENERATED FROM PYTHON SOURCE LINES 126-156

.. code-block:: Python



    async def example_streaming() -> None:
        """An example of using the streaming model."""
        model = DashScopeChatModel(
            model_name="qwen-max",
            api_key=os.environ["DASHSCOPE_API_KEY"],
            stream=True,
        )

        generator = await model(
            messages=[
                {
                    "role": "user",
                    "content": "Count from 1 to 20, and just report the number without any other information.",
                },
            ],
        )
        print("The type of the response:", type(generator))

        i = 0
        async for chunk in generator:
            print(f"Chunk {i}")
            print(f"\ttype: {type(chunk.content)}")
            print(f"\t{chunk}\n")
            i += 1


    asyncio.run(example_streaming())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The type of the response: <class 'async_generator'>
    Chunk 0
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1'}], id='2025-08-18 07:12:51.635_6de1c0', created_at='2025-08-18 07:12:51.635', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=1, time=2.081406, type='chat'))

    Chunk 1
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n'}], id='2025-08-18 07:12:51.681_1c3df7', created_at='2025-08-18 07:12:51.681', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=2, time=2.12767, type='chat'))

    Chunk 2
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2'}], id='2025-08-18 07:12:51.728_753f9f', created_at='2025-08-18 07:12:51.728', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=3, time=2.174851, type='chat'))

    Chunk 3
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n'}], id='2025-08-18 07:12:51.776_1f7600', created_at='2025-08-18 07:12:51.776', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=4, time=2.222072, type='chat'))

    Chunk 4
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n'}], id='2025-08-18 07:12:51.967_11988d', created_at='2025-08-18 07:12:51.967', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=8, time=2.413096, type='chat'))

    Chunk 5
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n'}], id='2025-08-18 07:12:52.155_5bd488', created_at='2025-08-18 07:12:52.155', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=12, time=2.601371, type='chat'))

    Chunk 6
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n'}], id='2025-08-18 07:12:52.345_2b8159', created_at='2025-08-18 07:12:52.345', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=16, time=2.791551, type='chat'))

    Chunk 7
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10'}], id='2025-08-18 07:12:52.664_430431', created_at='2025-08-18 07:12:52.664', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=20, time=3.110161, type='chat'))

    Chunk 8
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n'}], id='2025-08-18 07:12:52.832_7de405', created_at='2025-08-18 07:12:52.832', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=24, time=3.278648, type='chat'))

    Chunk 9
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1'}], id='2025-08-18 07:12:53.021_2cabfd', created_at='2025-08-18 07:12:53.021', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=28, time=3.467102, type='chat'))

    Chunk 10
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14'}], id='2025-08-18 07:12:53.248_59823d', created_at='2025-08-18 07:12:53.248', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=32, time=3.694537, type='chat'))

    Chunk 11
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n'}], id='2025-08-18 07:12:53.421_35f242', created_at='2025-08-18 07:12:53.421', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=36, time=3.867081, type='chat'))

    Chunk 12
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n1'}], id='2025-08-18 07:12:53.590_e95836', created_at='2025-08-18 07:12:53.590', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=40, time=4.035998, type='chat'))

    Chunk 13
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18'}], id='2025-08-18 07:12:53.779_0ad43b', created_at='2025-08-18 07:12:53.779', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=44, time=4.225118, type='chat'))

    Chunk 14
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n'}], id='2025-08-18 07:12:53.965_4c43b7', created_at='2025-08-18 07:12:53.965', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=48, time=4.41177, type='chat'))

    Chunk 15
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20'}], id='2025-08-18 07:12:54.103_4c76df', created_at='2025-08-18 07:12:54.103', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=50, time=4.549451, type='chat'))

    Chunk 16
            type: <class 'list'>
            ChatResponse(content=[{'type': 'text', 'text': '1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20'}], id='2025-08-18 07:12:54.123_55bbb1', created_at='2025-08-18 07:12:54.123', type='chat', usage=ChatUsage(input_tokens=27, output_tokens=50, time=4.569092, type='chat'))





.. GENERATED FROM PYTHON SOURCE LINES 157-161

Reasoning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
AgentScope supports reasoning models by providing the ``ThinkingBlock``.


.. GENERATED FROM PYTHON SOURCE LINES 161-186

.. code-block:: Python



    async def example_reasoning() -> None:
        """An example of using the reasoning model."""
        model = DashScopeChatModel(
            model_name="qwen-turbo",
            api_key=os.environ["DASHSCOPE_API_KEY"],
            enable_thinking=True,
        )

        res = await model(
            messages=[
                {"role": "user", "content": "Who am I?"},
            ],
        )

        last_chunk = None
        async for chunk in res:
            last_chunk = chunk
        print("The final response:")
        print(last_chunk)


    asyncio.run(example_reasoning())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The final response:
    ChatResponse(content=[{'type': 'thinking', 'thinking': 'Okay, the user asked "Who am I?" So, I need to figure out how to respond. First, I should consider that this is a common question people ask when they\'re trying to understand their identity or purpose. But since I\'m an AI, I don\'t have a personal identity. However, I should make sure to acknowledge their question and offer help.\n\nI should start by explaining that I can\'t know their personal identity, but I can assist them in exploring it. Maybe ask them to share more about themselves so I can provide better guidance. It\'s important to be empathetic and open-ended. Also, I need to keep the tone friendly and supportive. Let me check if there\'s any cultural context I should be aware of, but since it\'s a general question, probably not. I should avoid making assumptions and let them lead the conversation. Alright, that should cover it.'}, {'type': 'text', 'text': 'The question "Who am I?" is deeply personal and often philosophical, touching on identity, purpose, and self-awareness. As an AI, I donâ€™t have a personal identity or consciousness, but I can help you explore this question in a few ways:\n\n1. **Self-Reflection**: Think about your values, passions, experiences, and relationships. What makes you unique? What do you stand for?\n2. **Perspectives**: Philosophers, psychologists, and spiritual traditions offer different answers. For example:\n   - **Existentialist view**: You create your own meaning through choices and actions.\n   - **Psychological view**: Identity is shaped by memories, personality, and social roles.\n   - **Spiritual view**: You might see yourself as more than the physical selfâ€”connected to something greater.\n3. **Ask Yourself**: What are your goals, fears, and dreams? How do you want to grow?\n\nIf youâ€™d like, I can help you brainstorm or explore these ideas further. What resonates most with you? ðŸ˜Š'}], id='2025-08-18 07:13:01.228_f0af58', created_at='2025-08-18 07:13:01.228', type='chat', usage=ChatUsage(input_tokens=12, output_tokens=395, time=7.100258, type='chat'))




.. GENERATED FROM PYTHON SOURCE LINES 187-195

Tools API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Different model providers differ in their tools APIs, e.g. the tools JSON schema, the tool call/response format.
To provide a unified interface, AgentScope solves the problem by:

- Providing unified tool call block :ref:`ToolUseBlock <tool-block>` and tool response block :ref:`ToolResultBlock <tool-block>`, respectively.
- Providing a unified tools interface in the ``__call__`` method of the model classes, that accepts a list of tools JSON schemas as follows:


.. GENERATED FROM PYTHON SOURCE LINES 195-216

.. code-block:: Python


    json_schemas = [
        {
            "type": "function",
            "function": {
                "name": "google_search",
                "description": "Search for a query on Google.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The search query.",
                        },
                    },
                    "required": ["query"],
                },
            },
        },
    ]








.. GENERATED FROM PYTHON SOURCE LINES 217-222

Further Reading
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- :ref:`message`
- :ref:`prompt`



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 12.862 seconds)


.. _sphx_glr_download_tutorial_task_model.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: task_model.ipynb <task_model.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: task_model.py <task_model.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: task_model.zip <task_model.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
