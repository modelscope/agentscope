{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Retrieval Augmentation Generation (RAG)\n\nAgentscope has built-in supports for the retrieval augmentation generation\n(RAG). There are two key modules related to RAG in AgentScope: `Knowledge` and\n`KnowledgeBank`.\n\n## Create and Use Knowledge Instances\n\nWhile `Knowledge` is a base class, a specific built-in knowledge class is in\nthe AgentScope now. (Online search is coming soon.)\n\n\n- `LlamaIndexKnowledge`: Designed to work with one of the most popular RAG library [LlamaIndex](https://www.llamaindex.ai/) as local knowledge, and supporting most of LlamaIndex functionality by configuration.\n\n\n### Create a `LlamaIndexKnowledge` instance\n\nA quick start to create a `LlamaIndexKnowledge` instance is to use the `build_knowledge_instance` function.\nThere are three parameters need to be passed to the function.\n\n- `knowledge_id`: a unique identifier for this knowledge instance\n\n- `data_dirs_and_types`: a dictionary whose keys are strings of directories of the data, and values are the file extensions of the data\n\n- `emb_model_config_name`: name of the configuration of a embedding model in AgentScope (need to be initialized in AgentScope beforehand)\n\nA simple example is as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport agentscope\nfrom agentscope.rag.llama_index_knowledge import LlamaIndexKnowledge\n\nagentscope.init(\n    model_configs=[\n        {\n            \"model_type\": \"dashscope_text_embedding\",\n            \"config_name\": \"qwen_emb_config\",\n            \"model_name\": \"text-embedding-v2\",\n            \"api_key\": os.getenv(\"DASHSCOPE_API_KEY\"),\n        },\n    ],\n)\n\nlocal_knowledge = LlamaIndexKnowledge.build_knowledge_instance(\n    knowledge_id=\"agentscope_qa\",\n    data_dirs_and_types={\"./\": [\".md\"]},\n    emb_model_config_name=\"qwen_emb_config\",\n)\n\n\nnodes = local_knowledge.retrieve(\n    \"what is agentscope?\",\n    similarity_top_k=1,\n)\n\n\nprint(f\"\\nThe retrieved content:\\n{nodes[0].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If one wants to have more control on how the data are preprocessing,\na knowledge configuration can be passed to the function.\nEspecially, `SimpleDirectoryReader` is the class in LlamaIndex library, and `init_args` is the initialization parameters of `SimpleDirectoryReader`.\nAs for the data preprocessing, developers can choose different LlamaIndex [transformation operations](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations/) to preprocess the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flex_knowledge_config = {\n    \"knowledge_id\": \"agentscope_qa_flex\",\n    \"knowlege_type\": \"llamaindex_knowledge\",\n    \"emb_model_config_name\": \"qwen_emb_config\",\n    \"chunk_size\": 1024,\n    \"chunk_overlap\": 40,\n    \"data_processing\": [\n        {\n            \"load_data\": {\n                \"loader\": {\n                    \"create_object\": True,\n                    \"module\": \"llama_index.core\",\n                    \"class\": \"SimpleDirectoryReader\",\n                    \"init_args\": {\n                        \"input_dir\": \"./\",\n                        \"required_exts\": [\n                            \".md\",\n                        ],\n                    },\n                },\n            },\n            \"store_and_index\": {\n                \"transformations\": [\n                    {\n                        \"create_object\": True,\n                        \"module\": \"llama_index.core.node_parser\",\n                        \"class\": \"SentenceSplitter\",\n                        \"init_args\": {\n                            \"chunk_size\": 1024,\n                        },\n                    },\n                ],\n            },\n        },\n    ],\n}\n\nlocal_knowledge_flex = LlamaIndexKnowledge.build_knowledge_instance(\n    knowledge_id=\"agentscope_qa_flex\",\n    knowledge_config=flex_knowledge_config,\n)\n\n\nnodes = local_knowledge.retrieve(\n    \"what is agentscope?\",\n    similarity_top_k=1,\n)\n\nprint(f\"\\nThe retrieved content:\\n{nodes[0].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Batch of Knowledge Instances\nFor some cases where different knowledge sources exists and require different preprocessing and/or post-proprocess, a good strategy is to create multiple knolwedge instances.\nThus, we introduce `KnowledgeBank` to better manage the knowledge instances. One can initialize a batch of knowledge with a file of mulltiple knodledge configurations.\n\n```python\nknowledge_bank = KnowledgeBank(configs=path_to_knowledge_configs_json)\n```\nAlternatively, one can add knowledge instance dynamically to knowledge bank as well.\n\n```python\nknowledge_bank.add_data_as_knowledge(\n     knowledge_id=\"agentscope_tutorial_rag\",\n     emb_model_name=\"qwen_emb_config\",\n     data_dirs_and_types={\n         \"../../docs/sphinx_doc/en/source/tutorial\": [\".md\"],\n     },\n )\n```\nHere, the `knowledge_id` should be unique.\nIf developers have their new knowledge class, they can register the new class beforehand\n\n```python\nfrom your_knowledge import NewKnowledgeClass1, NewKnowledgeClass2\nknowledge_bank = KnowledgeBank(\n  configs=\"configs/knowledge_config.json\",\n  new_knowledge_types=[NewKnowledgeClass1, NewKnowledgeClass2]\n)\n# or\nknowledge_bank.register_knowledge_type(NewKnowledgeClass2)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Setting up a local embedding model service\n\nFor those who are interested in setting up a local embedding service, we provide the following example based on the\n`sentence_transformers` package, which is a popular specialized package for embedding models (based on the `transformer` package and compatible with both HuggingFace and ModelScope models).\nIn this example, we will use one of the SOTA embedding models, `gte-Qwen2-7B-instruct`.\n\n* Step 1: Follow the instruction on [HuggingFace](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct) or `ModelScope <https://www.modelscope.cn/models/iic/gte_Qwen2-7B-instruct >\"_ to download the embedding model.\n  (For those who cannot access HuggingFace directly, you may want to use a HuggingFace mirror by running a bash command\n    `export HF_ENDPOINT=https://hf-mirror.com` or add a line of code `os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"` in your Python code.)\n* Step 2: Set up the server. The following code is for reference.\n\n```python\nimport datetime\nimport argparse\n\nfrom flask import Flask\nfrom flask import request\nfrom sentence_transformers import SentenceTransformer\n\ndef create_timestamp(format_: str = \"%Y-%m-%d %H:%M:%S\") -> str:\n    \"\"\"Get current timestamp.\"\"\"\n    return datetime.datetime.now().strftime(format_)\n\napp = Flask(__name__)\n\n@app.route(\"/embedding/\", methods=[\"POST\"])\ndef get_embedding() -> dict:\n    \"\"\"Receive post request and return response\"\"\"\n    json = request.get_json()\n\n    inputs = json.pop(\"inputs\")\n\n    global model\n\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    embeddings = model.encode(inputs)\n\n    return {\n        \"data\": {\n            \"completion_tokens\": 0,\n            \"messages\": {},\n            \"prompt_tokens\": 0,\n            \"response\": {\n                \"data\": [\n                    {\n                        \"embedding\": emb.astype(float).tolist(),\n                    }\n                    for emb in embeddings\n                ],\n                \"created\": \"\",\n                \"id\": create_timestamp(),\n                \"model\": \"flask_model\",\n                \"object\": \"text_completion\",\n                \"usage\": {\n                    \"completion_tokens\": 0,\n                    \"prompt_tokens\": 0,\n                    \"total_tokens\": 0,\n                },\n            },\n            \"total_tokens\": 0,\n            \"username\": \"\",\n        },\n    }\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_name_or_path\", type=str, required=True)\n    parser.add_argument(\"--device\", type=str, default=\"auto\")\n    parser.add_argument(\"--port\", type=int, default=8000)\n    args = parser.parse_args()\n\n    global model\n\n    print(\"setting up for embedding model....\")\n    model = SentenceTransformer(\n        args.model_name_or_path\n    )\n\n    app.run(port=args.port)\n```\n* Step 3: start server.\n\n```bash\npython setup_ms_service.py --model_name_or_path {$PATH_TO_gte_Qwen2_7B_instruct}\n```\nTesting whether the model is running successfully.\n\n```python\nfrom agentscope.models.post_model import PostAPIEmbeddingWrapper\n\n\nmodel = PostAPIEmbeddingWrapper(\n    config_name=\"test_config\",\n    api_url=\"http://127.0.0.1:8000/embedding/\",\n    json_args={\n        \"max_length\": 4096,\n        \"temperature\": 0.5\n    }\n)\n\nprint(model(\"testing\"))\n```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}