{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Model APIs\n\nAgentScope has integrated many popular model API libraries with different modalities.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>1. The text-to-speech (TTS) and speech-to-text (STT) APIs are not included in this list. You can refer to the section `tools`.</p></div>\n\n 2. The section only introduces how to use or integrate different model APIs in AgentScope. The prompt requirements and prompt engineering strategies are left in the section `prompt-engineering`.\n\n\n.. list-table::\n    :header-rows: 1\n\n    * - API\n      - Chat\n      - Text Generation\n      - Vision\n      - Embedding\n    * - OpenAI\n      - \u2713\n      - \u2717\n      - \u2713\n      - \u2713\n    * - DashScope\n      - \u2713\n      - \u2717\n      - \u2713\n      - \u2713\n    * - Gemini\n      - \u2713\n      - \u2717\n      - \u2717\n      - \u2713\n    * - Ollama\n      - \u2713\n      - \u2713\n      - \u2713\n      - \u2713\n    * - Yi\n      - \u2713\n      - \u2717\n      - \u2717\n      - \u2717\n    * - LiteLLM\n      - \u2713\n      - \u2717\n      - \u2717\n      - \u2717\n    * - Zhipu\n      - \u2713\n      - \u2717\n      - \u2717\n      - \u2713\n    * - Anthropic\n      - \u2713\n      - \u2717\n      - \u2717\n      - \u2717\n\nThere are two ways to use the model APIs in AgentScope. You can choose the one that suits you best.\n\n- **Use Configuration**: This is the recommended way to build model API-agnostic applications. You can change model API by modifying the configuration, without changing the code.\n- **Initialize Model Explicitly**: If you only want to use a specific model API, initialize model explicitly is much more convenient and transparent to the developer. The API docstrings provide detailed information on the parameters and usage.\n\n.. tip:: Actually, using configuration and initializing model explicitly are equivalent. When you use the configuration, AgentScope just passes the key-value pairs in the configuration to initialize the model automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom agentscope.models import (\n    DashScopeChatWrapper,\n    ModelWrapperBase,\n    ModelResponse,\n)\nimport agentscope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Configuration\nIn a model configuration, the following three fields are required:\n\n- config_name: The name of the configuration.\n- model_type: The type of the model API, e.g. \"dashscope_chat\", \"openai_chat\", etc.\n- model_name: The name of the model, e.g. \"qwen-max\", \"gpt-4o\", etc.\n\nYou should load the configurations before using the model APIs by calling `agentscope.init()` as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "agentscope.init(\n    model_configs=[\n        {\n            \"config_name\": \"gpt-4o_temperature-0.5\",\n            \"model_type\": \"openai_chat\",\n            \"model_name\": \"gpt-4o\",\n            \"api_key\": \"xxx\",\n            \"temperature\": 0.5,\n        },\n        {\n            \"config_name\": \"my-qwen-max\",\n            \"model_type\": \"dashscope_chat\",\n            \"model_name\": \"qwen-max\",\n        },\n    ],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the other parameters, you\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing Model Explicitly\nThe available model APIs are modularized in the `agentscope.models` module.\nYou can initialize a model explicitly by calling the corresponding model class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# print the modules under agentscope.models\nfor module_name in agentscope.models.__all__:\n    if module_name.endswith(\"Wrapper\"):\n        print(module_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking DashScope Chat API as an example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = DashScopeChatWrapper(\n    config_name=\"_\",\n    model_name=\"qwen-max\",\n    api_key=os.environ[\"DASHSCOPE_API_KEY\"],\n    stream=False,\n)\n\nresponse = model(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hi!\"},\n    ],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `response` is an object of `agentscope.models.ModelResponse`, which contains the following fields:\n\n- text: The generated text\n- embedding: The generated embeddings\n- image_urls: Refer to generated images\n- raw: The raw response from the API\n- parsed: The parsed response, e.g. load the text into a JSON object\n- stream: A generator that yields the response text chunk by chunk, refer to section :ref: `streaming` for more details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Text: {response.text}\")\nprint(f\"Embedding: {response.embedding}\")\nprint(f\"Image URLs: {response.image_urls}\")\nprint(f\"Raw: {response.raw}\")\nprint(f\"Parsed: {response.parsed}\")\nprint(f\"Stream: {response.stream}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Integrating New LLM API\nThere are two ways to integrate a new LLM API into AgentScope.\n\n### OpenAI-Compatible APIs\n\nIf your model is compatible with OpenAI Python API, reusing the `OpenAIChatWrapper` class with specific parameters is recommended.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You should take care of the messages format manually.</p></div>\n\nTaking [vLLM](https://github.com/vllm-project/vllm?tab=readme-ov-file), an OpenAI-compatible LLM inference engine, as an example,\nits [official doc](https://github.com/vllm-project/vllm?tab=readme-ov-file) provides the following example for OpenAI Python client library:\n\n```python\nfrom openai import OpenAI\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"token-abc123\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.5,\n)\n\nprint(completion.choices[0].message)\n```\nIt's very easy to integrate vLLM into AgentScope as follows:\n\n- put the parameters for initializing OpenAI client (except `api_key`) into `client_args`, and\n- the parameters for generating completions (expect `model`) into `generate_args`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vllm_model_config = {\n    \"model_type\": \"openai_chat\",\n    \"config_name\": \"vllm_llama2-7b-chat-hf\",\n    \"model_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"api_key\": \"token-abc123\",  # The API key\n    \"client_args\": {\n        \"base_url\": \"http://localhost:8000/v1/\",  # Used to specify the base URL of the API\n    },\n    \"generate_args\": {\n        \"temperature\": 0.5,  # The generation parameters, e.g. temperature, seed\n    },\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, directly initialize the OpenAI Chat model wrapper with the parameters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.models import OpenAIChatWrapper\n\nmodel = OpenAIChatWrapper(\n    config_name=\"\",\n    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n    api_key=\"token-abc123\",\n    client_args={\"base_url\": \"http://localhost:8000/v1/\"},\n    generate_args={\"temperature\": 0.5},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RESTful APIs\n\nIf your model is accessed via RESTful post API, and OpenAI-compatible in response format, consider to use the `PostAPIChatWrapper`.\n\nTaking the following curl command as an example, just extract the **header**, **API URL**, and **data** (except `messages`, which will be passed automatically) as the parameters for initializing the model wrapper.\n\nFor an example post request:\n\n```bash\ncurl https://api.openai.com/v1/chat/completions\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer $OPENAI_API_KEY\"\n-d '{\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n        ]\n    }'\n```\nThe corresponding model wrapper initialization is as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.models import PostAPIChatWrapper\n\npost_api_model = PostAPIChatWrapper(\n    config_name=\"\",\n    api_url=\"https://api.openai.com/v1/chat/completions\",  # The target URL\n    headers={\n        \"Content-Type\": \"application/json\",  # From the header\n        \"Authorization\": \"Bearer $OPENAI_API_KEY\",\n    },\n    json_args={\n        \"model\": \"gpt-4o\",  # From the data\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Its model configuration is as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "post_api_config = {\n    \"config_name\": \"{my_post_model_config_name}\",\n    \"model_type\": \"post_api_chat\",\n    \"api_url\": \"https://api.openai.com/v1/chat/completions\",\n    \"headers\": {\n        \"Authorization\": \"Bearer {YOUR_API_TOKEN}\",\n    },\n    \"json_args\": {\n        \"model\": \"gpt-4o\",\n    },\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your model API response format is different from OpenAI API, you can inherit from `PostAPIChatWrapper` and override the `_parse_response` method to adapt to your API response format.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You need to define a new `model_type` field in the subclass to distinguish it from the existing model wrappers.</p></div>\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyNewModelWrapper(PostAPIChatWrapper):\n    model_type: str = \"{my_new_model_type}\"\n\n    def _parse_response(self, response: dict) -> ModelResponse:\n        \"\"\"Parse the response from the API server.\n\n        Args:\n            response (`dict`):\n                The response obtained from API server and parsed by\n                `response.json()` to unify the format.\n\n        Return (`ModelResponse`):\n            The parsed response.\n        \"\"\"\n        # TODO: Replace by your own parsing logic\n        return ModelResponse(\n            text=response[\"data\"][\"response\"][\"choices\"][0][\"message\"][\n                \"content\"\n            ],\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From Scratch\n\nIf you decide to implement a new model API from scratch, you need to know the following concepts in AgentScope:\n\n- **model_type**: When using model configurations, AgentScope uses the `model_type` field to distinguish different model APIs. So ensure your new model wrapper class has a unique `model_type`.\n- **__init__**: When initializing from configuration, AgentScope passes all the key-value pairs in the configuration to the `__init__` method of the model wrapper. So ensure your `__init__` method can handle all the parameters in the configuration.\n- **__call__**: The core method of the model wrapper is `__call__`, which takes the input messages and returns the response. Its return value should be an object of `ModelResponse`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyNewModelWrapper(ModelWrapperBase):\n    model_type: str = \"{my_new_model_type}\"\n\n    def __init__(self, config_name, model_name, **kwargs) -> None:\n        super().__init__(config_name, model_name=model_name)\n\n        # TODO: Initialize your model here\n\n    def __call__(self, *args, **kwargs) -> ModelResponse:\n        # TODO: Implement the core logic of your model here\n\n        return ModelResponse(\n            text=\"Hello, World!\",\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. tip:: Optionally, you can implement a format method to format the prompt before sending it to the model API.\n Refer to `prompt-engineering` for more details.\n\n## Further Reading\n- `prompt-engineering`\n- `streaming`\n- `structured-output`\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}