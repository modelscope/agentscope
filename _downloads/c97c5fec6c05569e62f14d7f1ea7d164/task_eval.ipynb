{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Evaluation\n\nAgentScope provides a built-in evaluation framework for assessing agent performance across different tasks and benchmarks, featuring:\n\n- [Ray](https://github.com/ray-project/ray)-based parallel and distributed evaluation\n- Support continuation after interruption\n- \ud83d\udea7 Visualization of evaluation results\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We are keeping integrating new benchmarks into AgentScope:</p></div>\n\n - \u2705 [ACEBench](https://github.com/ACEBench/ACEBench)\n - \ud83d\udea7 [GAIA](https://huggingface.co/datasets/gaia-benchmark/GAIA/tree/main) Benchmark\n\n\n## Overview\n\nThe AgentScope evaluation framework consists of several key components:\n\n- **Benchmark**: Collections of tasks for systematic evaluation\n    - **Task**: Individual evaluation units with inputs, ground truth, and metrics\n        - **Metric**: Measurement functions that assess solution quality\n- **Evaluator**: Engine that runs evaluation, aggregates results, and analyzes performance\n    - **Evaluator Storage**: Persistent storage for recording and retrieving evaluation results\n- **Solution**: The user-defined solution\n\n.. figure:: ../../_static/images/evaluation.png\n    :width: 90%\n    :alt: AgentScope Evaluation Framework\n\n    *AgentScope Evaluation Framework*\n\nThe current implementation in AgentScope includes:\n\n- Evaluator:\n    - ``RayEvaluator``: A ray-based evaluator that supports parallel and distributed evaluation.\n    - ``GeneralEvaluator``: A general evaluator that runs tasks sequentially, friendly for debugging.\n- Benchmark:\n    - ``ACEBench``: A benchmark for evaluating agent capabilities.\n\nWe have provided a toy example in our [GitHub repository](https://github.com/agentscope-ai/agentscope/tree/main/examples/evaluation/ace_bench) with ``RayEvaluator`` and the agent multistep tasks in ACEBench.\n\n## Core Components\nWe are going to build a simple toy math question benchmark to demonstrate\nhow to use the AgentScope evaluation module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "TOY_BENCHMARK = [\n    {\n        \"id\": \"math_problem_1\",\n        \"question\": \"What is 2 + 2?\",\n        \"ground_truth\": 4.0,\n        \"tags\": {\n            \"difficulty\": \"easy\",\n            \"category\": \"math\",\n        },\n    },\n    {\n        \"id\": \"math_problem_2\",\n        \"question\": \"What is 12345 + 54321 + 6789 + 9876?\",\n        \"ground_truth\": 83331,\n        \"tags\": {\n            \"difficulty\": \"medium\",\n            \"category\": \"math\",\n        },\n    },\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From Tasks, Solutions and Metrics to Benchmark\n\n- A ``SolutionOutput`` contains all information generated by the agent, including the trajectory and final output.\n- A ``Metric`` represents a single evaluation callable instance that compares the generated solution (e.g., trajectory or final output) to the ground truth.\nIn the toy example, we define a metric that simply checks whether the ``output`` field in the solution matches the ground truth.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.evaluate import (\n    SolutionOutput,\n    MetricBase,\n    MetricResult,\n    MetricType,\n)\n\n\nclass CheckEqual(MetricBase):\n    def __init__(\n        self,\n        ground_truth: float,\n    ):\n        super().__init__(\n            name=\"math check number equal\",\n            metric_type=MetricType.NUMERICAL,\n            description=\"Toy metric checking if two numbers are equal\",\n            categories=[],\n        )\n        self.ground_truth = ground_truth\n\n    def __call__(\n        self,\n        solution: SolutionOutput,\n    ) -> MetricResult:\n        if solution.output == self.ground_truth:\n            return MetricResult(\n                name=self.name,\n                result=1.0,\n                message=\"Correct\",\n            )\n        else:\n            return MetricResult(\n                name=self.name,\n                result=0.0,\n                message=\"Incorrect\",\n            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- A ``Task`` is a unit in the benchmark that includes all information for the agent to execute and evaluate (e.g., input/query and its ground truth).\n- A ``Benchmark`` organizes multiple tasks for systematic evaluation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Generator\nfrom agentscope.evaluate import (\n    Task,\n    BenchmarkBase,\n)\n\n\nclass ToyBenchmark(BenchmarkBase):\n    def __init__(self):\n        super().__init__(\n            name=\"Toy bench\",\n            description=\"A toy benchmark for demonstrating the evaluation module.\",\n        )\n        self.dataset = self._load_data()\n\n    @staticmethod\n    def _load_data() -> list[Task]:\n        dataset = []\n        for item in TOY_BENCHMARK:\n            dataset.append(\n                Task(\n                    id=item[\"id\"],\n                    input=item[\"question\"],\n                    ground_truth=item[\"ground_truth\"],\n                    tags=item.get(\"tags\", {}),\n                    metrics=[\n                        CheckEqual(item[\"ground_truth\"]),\n                    ],\n                    metadata={},\n                ),\n            )\n        return dataset\n\n    def __iter__(self) -> Generator[Task, None, None]:\n        \"\"\"Iterate over the benchmark.\"\"\"\n        for task in self.dataset:\n            yield task\n\n    def __getitem__(self, index: int) -> Task:\n        \"\"\"Get a task by index.\"\"\"\n        return self.dataset[index]\n\n    def __len__(self) -> int:\n        \"\"\"Get the length of the benchmark.\"\"\"\n        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluators\n\nEvaluators manage the evaluation process. They can automatically iterate through the\ntasks in the benchmark and feed each task into a solution-generation function,\nwhere developers need to define the logic for running agents and retrieving\nthe execution result and trajectory. Below is an example of\nrunning ``GeneralEvaluator`` with our toy benchmark. If there is a large\nbenchmark and the developer wants to get the evaluation more efficiently\nthrough parallelization, ``RayEvaluator`` is available as a built-in solution\nas well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport asyncio\nfrom typing import Callable\nfrom pydantic import BaseModel\n\nfrom agentscope.message import Msg\nfrom agentscope.model import DashScopeChatModel\nfrom agentscope.formatter import DashScopeChatFormatter\nfrom agentscope.agent import ReActAgent\n\nfrom agentscope.evaluate import (\n    GeneralEvaluator,\n    FileEvaluatorStorage,\n)\n\n\nclass ToyBenchAnswerFormat(BaseModel):\n    answer_as_number: float\n\n\nasync def toy_solution_generation(\n    task: Task,\n    pre_hook: Callable,\n) -> SolutionOutput:\n    agent = ReActAgent(\n        name=\"Friday\",\n        sys_prompt=\"You are a helpful assistant named Friday. \"\n        \"Your target is to solve the given task with your tools. \"\n        \"Try to solve the task as best as you can.\",\n        model=DashScopeChatModel(\n            api_key=os.environ.get(\"DASHSCOPE_API_KEY\"),\n            model_name=\"qwen-max\",\n            stream=False,\n        ),\n        formatter=DashScopeChatFormatter(),\n    )\n    agent.register_instance_hook(\n        \"pre_print\",\n        \"save_logging\",\n        pre_hook,\n    )\n    msg_input = Msg(\"user\", task.input, role=\"user\")\n    res = await agent(\n        msg_input,\n        structured_model=ToyBenchAnswerFormat,\n    )\n    return SolutionOutput(\n        success=True,\n        output=res.metadata.get(\"answer_as_number\", None),\n        trajectory=[],\n    )\n\n\nasync def main() -> None:\n    evaluator = GeneralEvaluator(\n        name=\"ACEbench evaluation\",\n        benchmark=ToyBenchmark(),\n        # Repeat how many times\n        n_repeat=1,\n        storage=FileEvaluatorStorage(\n            save_dir=\"./results\",\n        ),\n        # How many workers to use\n        n_workers=1,\n    )\n\n    # Run the evaluation\n    await evaluator.run(toy_solution_generation)\n\n\nasyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}