<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Different Model Sources with Model API &mdash; AgentScope  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Enhancing Agent Capabilities with Service Functions" href="204-service.html" />
    <link rel="prev" title="Agent Interactions: Dive deeper into Pipelines and Message Hub" href="202-pipeline.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            AgentScope
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">AgentScope Tutorial</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="advance.html">Advanced Exploration</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="201-agent.html">Customizing Your Own Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="202-pipeline.html">Agent Interactions: Dive deeper into Pipelines and Message Hub</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Different Model Sources with Model API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#standard-openai-api">Standard OpenAI API</a></li>
<li class="toctree-l3"><a class="reference internal" href="#self-host-model-api">Self-host Model API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#flask-based-model-api-serving">Flask-based Model API Serving</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fastchat">FastChat</a></li>
<li class="toctree-l4"><a class="reference internal" href="#vllm">vllm</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-inference-api">Model Inference API</a></li>
<li class="toctree-l3"><a class="reference internal" href="#in-memory-models-without-api">In-memory Models without API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="204-service.html">Enhancing Agent Capabilities with Service Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="205-memory.html">Memory and Message Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="206-prompt.html">Prompt Engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="207-monitor.html">Monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="208-distribute.html">Make Your Applications Distributed</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">Get Involved</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AgentScope API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.agents.html">Agents package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.memory.html">Memory package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.models.html">Models package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.pipelines.html">Pipelines package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.service.html">Service package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.rpc.html">RPC package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.utils.html">Utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.web.html">Web UI package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../agentscope.html">Module contents</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AgentScope</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="advance.html">Advanced Exploration</a></li>
      <li class="breadcrumb-item active">Using Different Model Sources with Model API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorial/203-model.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-different-model-sources-with-model-api">
<span id="model"></span><h1>Using Different Model Sources with Model API<a class="headerlink" href="#using-different-model-sources-with-model-api" title="Link to this heading"></a></h1>
<p>AgentScope allows for the integration of multi-modal models from various sources. The core step is the initialization process, where once initialized with a certain config, all agent instances globally select the appropriate model APIs based on the model name specified (e.g., <code class="docutils literal notranslate"><span class="pre">model='gpt-4'</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">agentscope</span>

<span class="n">agentscope</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">model_configs</span><span class="o">=</span><span class="n">PATH_TO_MODEL_CONFIG</span><span class="p">)</span>
</pre></div>
</div>
<p>where the model configs could be a list of dict:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;config_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt-4-temperature-0.0&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;model_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;openai&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;model_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxx&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;organization&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxx&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;generate_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;config_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;dall-e-3-size-1024x1024&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;model_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;openai_dall_e&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;model_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;dall-e-3&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxx&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;organization&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxx&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;generate_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1024x1024&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="c1">// Additional models can be configured here</span>
<span class="p">]</span>
</pre></div>
</div>
<p>This allows users to configure the model once, enabling shared use across all agents within the multi-agent application. Here is a table outlining the supported APIs and the type of arguments required for each:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model Usage</p></th>
<th class="head"><p>Type Argument in AgentScope</p></th>
<th class="head"><p>Supported APIs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Text generation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">openai</span></code></p></td>
<td><p>Standard OpenAI chat API, FastChat and vllm</p></td>
</tr>
<tr class="row-odd"><td><p>Image generation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">openai_dall_e</span></code></p></td>
<td><p>DALL-E API for generating images</p></td>
</tr>
<tr class="row-even"><td><p>Embedding</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">openai_embedding</span></code></p></td>
<td><p>API for text embeddings</p></td>
</tr>
<tr class="row-odd"><td><p>General usages in POST</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">post_api</span></code></p></td>
<td><p>Huggingface/ModelScope Inference API, and customized post API</p></td>
</tr>
</tbody>
</table>
<section id="standard-openai-api">
<h2>Standard OpenAI API<a class="headerlink" href="#standard-openai-api" title="Link to this heading"></a></h2>
<p>Our configuration is fully compatible with the Standard OpenAI API. For specific parameter configuration and usage guides, we recommend visiting their official website: <a class="reference external" href="https://platform.openai.com/docs/api-reference/introduction">https://platform.openai.com/docs/api-reference/introduction</a>.</p>
</section>
<section id="self-host-model-api">
<h2>Self-host Model API<a class="headerlink" href="#self-host-model-api" title="Link to this heading"></a></h2>
<p>In AgentScope, in addition to OpenAI API, we also support open-source models with post-request API. In this document, we will introduce how to fast set up local model API serving with different inference engines.</p>
<section id="flask-based-model-api-serving">
<h3>Flask-based Model API Serving<a class="headerlink" href="#flask-based-model-api-serving" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://github.com/pallets/flask">Flask</a> is a lightweight web application framework. It is easy to build a local model API serving with Flask.</p>
<p>Here we provide two Flask examples with Transformers and ModelScope libraries, respectively. You can build your own model API serving with a few modifications.</p>
<section id="with-transformers-library">
<h4>With Transformers Library<a class="headerlink" href="#with-transformers-library" title="Link to this heading"></a></h4>
<section id="install-libraries-and-set-up-serving">
<h5>Install Libraries and Set up Serving<a class="headerlink" href="#install-libraries-and-set-up-serving" title="Link to this heading"></a></h5>
<p>Install Flask and Transformers by following the command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>Flask,<span class="w"> </span>transformers
</pre></div>
</div>
<p>Taking model <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-chat-hf</span></code> and port <code class="docutils literal notranslate"><span class="pre">8000</span></code> as an example, set up the model API serving by running the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>flask_transformers/setup_hf_service.py
<span class="w">    </span>--model_name_or_path<span class="w"> </span>meta-llama/Llama-2-7b-chat-hf
<span class="w">    </span>--device<span class="w"> </span><span class="s2">&quot;cuda:0&quot;</span><span class="w"> </span><span class="c1"># or &quot;cpu&quot;</span>
<span class="w">    </span>--port<span class="w"> </span><span class="m">8000</span>
</pre></div>
</div>
<p>You can replace <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-chat-hf</span></code> with any model card in the huggingface model hub.</p>
</section>
<section id="how-to-use-in-agentscope">
<h5>How to use in AgentScope<a class="headerlink" href="#how-to-use-in-agentscope" title="Link to this heading"></a></h5>
<p>In AgentScope, you can load the model with the following model configs: <code class="docutils literal notranslate"><span class="pre">./flask_transformers/model_config.json</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;model_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;post_api&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;flask_llama2-7b-chat&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;api_url&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;http://127.0.0.1:8000/llm/&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;json_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;max_length&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4096</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.5</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="note">
<h5>Note<a class="headerlink" href="#note" title="Link to this heading"></a></h5>
<p>In this model serving, the messages from post requests should be in <strong>STRING</strong> format. You can use <a class="reference external" href="https://huggingface.co/docs/transformers/main/chat_templating">templates for chat model</a> from <em>transformers</em> with a little modification based on <code class="docutils literal notranslate"><span class="pre">./flask_transformers/setup_hf_service.py</span></code>.</p>
</section>
</section>
<section id="with-modelscope-library">
<h4>With ModelScope Library<a class="headerlink" href="#with-modelscope-library" title="Link to this heading"></a></h4>
<section id="id1">
<h5>Install Libraries and Set up Serving<a class="headerlink" href="#id1" title="Link to this heading"></a></h5>
<p>Install Flask and modelscope by following the command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>Flask,<span class="w"> </span>modelscope
</pre></div>
</div>
<p>Taking model <code class="docutils literal notranslate"><span class="pre">modelscope/Llama-2-7b-ms</span></code> and port <code class="docutils literal notranslate"><span class="pre">8000</span></code> as an example, to set up the model API serving, run the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>flask_modelscope/setup_ms_service.py
<span class="w">    </span>--model_name_or_path<span class="w"> </span>modelscope/Llama-2-7b-ms
<span class="w">    </span>--device<span class="w"> </span><span class="s2">&quot;cuda:0&quot;</span><span class="w"> </span><span class="c1"># or &quot;cpu&quot;</span>
<span class="w">    </span>--port<span class="w"> </span><span class="m">8000</span>
</pre></div>
</div>
<p>You can replace <code class="docutils literal notranslate"><span class="pre">modelscope/Llama-2-7b-ms</span></code> with any model card in modelscope model hub.</p>
</section>
<section id="how-to-use-agentscope">
<h5>How to use AgentScope<a class="headerlink" href="#how-to-use-agentscope" title="Link to this heading"></a></h5>
<p>In AgentScope, you can load the model with the following model configs: <code class="docutils literal notranslate"><span class="pre">flask_modelscope/model_config.json</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;model_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;post_api&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;config_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;flask_llama2-7b-ms&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;api_url&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;http://127.0.0.1:8000/llm/&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;json_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;max_length&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4096</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.5</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id2">
<h5>Note<a class="headerlink" href="#id2" title="Link to this heading"></a></h5>
<p>Similar to the example of transformers, the messages from post requests should be in <strong>STRING format</strong>.</p>
</section>
</section>
</section>
<section id="fastchat">
<h3>FastChat<a class="headerlink" href="#fastchat" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://github.com/lm-sys/FastChat">FastChat</a> is an open platform that provides a quick setup for model serving with OpenAI-compatible RESTful APIs.</p>
<section id="id3">
<h4>Install Libraries and Set up Serving<a class="headerlink" href="#id3" title="Link to this heading"></a></h4>
<p>To install FastChat, run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;fastchat[model_worker,webui]&quot;</span>
</pre></div>
</div>
<p>Taking model <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-chat-hf</span></code> and port <code class="docutils literal notranslate"><span class="pre">8000</span></code> as an example, to set up model API serving, run the following command to set up model serving.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>fastchat_script/fastchat_setup.sh<span class="w"> </span>-m<span class="w"> </span>meta-llama/Llama-2-7b-chat-hf<span class="w"> </span>-p<span class="w"> </span><span class="m">8000</span>
</pre></div>
</div>
</section>
<section id="supported-models">
<h4>Supported Models<a class="headerlink" href="#supported-models" title="Link to this heading"></a></h4>
<p>Refer to <a class="reference external" href="https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md#supported-models">supported model list</a> of FastChat.</p>
</section>
<section id="id4">
<h4>How to use in AgentScope<a class="headerlink" href="#id4" title="Link to this heading"></a></h4>
<p>Now you can load the model in AgentScope by the following model config: <code class="docutils literal notranslate"><span class="pre">fastchat_script/model_config.json</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;config_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;model_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;openai&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;EMPTY&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;client_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;base_url&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;http://127.0.0.1:8000/v1/&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;generate_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.5</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="vllm">
<h3>vllm<a class="headerlink" href="#vllm" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://github.com/vllm-project/vllm">vllm</a> is a high-throughput inference and serving engine for LLMs.</p>
<section id="id5">
<h4>Install Libraries and Set up Serving<a class="headerlink" href="#id5" title="Link to this heading"></a></h4>
<p>To install vllm, run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>vllm
</pre></div>
</div>
<p>Taking model <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-chat-hf</span></code> and port <code class="docutils literal notranslate"><span class="pre">8000</span></code> as an example, to set up model API serving, run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>vllm_script/vllm_setup.sh<span class="w"> </span>-m<span class="w"> </span>meta-llama/Llama-2-7b-chat-hf<span class="w"> </span>-p<span class="w"> </span><span class="m">8000</span>
</pre></div>
</div>
</section>
<section id="id6">
<h4>Supported models<a class="headerlink" href="#id6" title="Link to this heading"></a></h4>
<p>Please refer to the <a class="reference external" href="https://docs.vllm.ai/en/latest/models/supported_models.html">supported models list</a> of vllm.</p>
</section>
<section id="id7">
<h4>How to use in AgentScope<a class="headerlink" href="#id7" title="Link to this heading"></a></h4>
<p>Now you can load the model in AgentScope by the following model config: <code class="docutils literal notranslate"><span class="pre">vllm_script/model_config.json</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;config_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;model_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;openai&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;EMPTY&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;client_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;base_url&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;http://127.0.0.1:8000/v1/&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;generate_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.5</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="model-inference-api">
<h2>Model Inference API<a class="headerlink" href="#model-inference-api" title="Link to this heading"></a></h2>
<p>Both <a class="reference external" href="https://huggingface.co/docs/api-inference/index">Huggingface</a> and <a class="reference external" href="https://www.modelscope.cn">ModelScope</a> provide model inference API, which can be used with AgentScope post API model wrapper.
Taking <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> in HuggingFace inference API as an example, you can use the following model config in AgentScope.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;config_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;model_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;post_api&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;headers&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;Authorization&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Bearer {YOUR_API_TOKEN}&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="nt">&quot;api_url&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="in-memory-models-without-api">
<h2>In-memory Models without API<a class="headerlink" href="#in-memory-models-without-api" title="Link to this heading"></a></h2>
<p>It is entirely possible to use models without setting up an API service. Here’s an example of how to initialize an agent with a local model instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># Do remember to re-implement the `reply` method to tokenize *message*!</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">YourAgent</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;agent&#39;</span><span class="p">,</span> <span class="n">model_config_name</span><span class="o">=</span><span class="n">config_name</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#using-different-model-sources-with-model-api"><span class="xref myst">[Return to the top]</span></a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="202-pipeline.html" class="btn btn-neutral float-left" title="Agent Interactions: Dive deeper into Pipelines and Message Hub" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="204-service.html" class="btn btn-neutral float-right" title="Enhancing Agent Capabilities with Service Functions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Alibaba Tongyi Lab.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>